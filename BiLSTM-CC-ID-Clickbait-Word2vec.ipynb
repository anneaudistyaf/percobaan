{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_train = pd.read_csv(\"Data Skripsi Anne NoTgl.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_train2, test= train_test_split(cod_train, random_state=0, test_size=0.2)\n",
    "train, val =  train_test_split(cod_train2, random_state = 0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83627b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Isi Berita'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1ecf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/candra/miniconda3/envs/env-wrap/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/candra/miniconda3/envs/env-wrap/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/candra/miniconda3/envs/env-wrap/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "max_features = 100000 # max num words\n",
    "maxlen = 250 \n",
    "embedding_size = 200\n",
    "\n",
    "# create the tokenizer with the maximum number of words to keep, \n",
    "# based on word frequency. \n",
    "# Only the most common num_words-1 words will be kept.\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
    "\n",
    "train['Isi Berita']=train['Isi Berita'].astype(str)\n",
    "test['Isi Berita']=test['Isi Berita'].astype(str)\n",
    "val['Isi Berita']=val['Isi Berita'].astype(str)\n",
    "\n",
    "# fit the tokenizer on the headlines\n",
    "tokenizer.fit_on_texts(list(train['Isi Berita']))\n",
    "\n",
    "# Transforms each text in texts to a sequence of integers.\n",
    "train_X = tokenizer.texts_to_sequences(train['Isi Berita'])\n",
    "test_X = tokenizer.texts_to_sequences(test['Isi Berita'])\n",
    "val_X = tokenizer.texts_to_sequences(val['Isi Berita'])\n",
    "\n",
    "# transforms a list of num_samples sequences (lists of integers)\n",
    "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
    "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
    "\n",
    "train_y = train['Label']\n",
    "test_y = test['Label']\n",
    "val_y = val['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebef064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [07:28, 4455.75it/s] "
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "Bigger_list=[]\n",
    "with open('vocab.txt','r') as f:\n",
    "    for line in f:\n",
    "        strip_lines=line.strip()\n",
    "        Bigger_list.append(strip_lines.split())\n",
    "\n",
    "model= Word2Vec(Bigger_list)\n",
    "model.save(\"word2vec.model\")\n",
    "model.save(\"model.bin\")\n",
    "\n",
    "load_model = Word2Vec.load('model.bin')\n",
    "\n",
    "embedding_matrix = load_model.wv.vectors\n",
    "vocab_size, emdedding_size = embedding_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, emdedding_size, weights = [embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(40, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be006a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after every epoch.\n",
    "saveBestModel = keras.callbacks.ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# Stop training when a monitored quantity has stopped improving.\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 25\n",
    "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9df784",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_pr=precision*recall\n",
    "sum_pr=precision+recall\n",
    "div=mult_pr/sum_pr\n",
    "f1_score=2*div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14736bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss:',loss)\n",
    "print('Accuracy:',accuracy)\n",
    "print('Precision:',precision)\n",
    "print('Recall:',recall)\n",
    "print('f1 score:',f1_score)\n",
    "print('True positives:',true_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714099fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict_classes(test_X, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6128542",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFP_FN_TP_lists(test_X, test_y, pred_y):\n",
    "    FP_text = []\n",
    "    FP_index = []\n",
    "    FN_text = []\n",
    "    FN_index = []\n",
    "    TP_text = []\n",
    "    TP_index = []\n",
    "    for i in range(len(test_y)):\n",
    "        if(pred_y[i]==1 and test_y[test_y.index[i]]==0):\n",
    "            FP_text.append(test['text'][test_y.index[i]])\n",
    "            FP_index.append(test_y.index[i])\n",
    "        elif(pred_y[i]==0 and test_y[test_y.index[i]]==1):\n",
    "            FN_text.append(test['text'][test_y.index[i]])\n",
    "            FN_index.append(test_y.index[i])\n",
    "        elif(pred_y[i]==1 and test_y[test_y.index[i]]==1):\n",
    "            TP_text.append(test['text'][test_y.index[i]])\n",
    "            TP_index.append(test_y.index[i])        \n",
    "            \n",
    "    return FP_text,FP_index,FN_text,FN_index,TP_text,TP_index\n",
    "\n",
    "def getFP_FN_TP(test_X, test_y, pred_y):\n",
    "    FP_text,FP_index,FN_text,FN_index,TP_text,TP_index = getFP_FN_TP_lists(test_X, test_y, pred_y)\n",
    "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
    "    df_FP = pd.DataFrame(d_FP)\n",
    "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
    "    df_FN = pd.DataFrame(d_FN)\n",
    "    d_TP =  {'TP_text':TP_text,'TP_index':TP_index}\n",
    "    df_TP = pd.DataFrame(d_TP)\n",
    "    \n",
    "    return df_FP,df_FN,df_TP\n",
    "\n",
    "df_FP,df_FN, df_TP = getFP_FN_TP(test_X, test_y, pred_y)\n",
    "df_FP.to_csv('FP_BiLSTMG_CB.csv', index=True)\n",
    "df_FN.to_csv('FN_BiLSTM_CB.csv', index=True)\n",
    "df_TP.to_csv('TP_BiLSTM_CB.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e764c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
