{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9966546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b5a34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_train = pd.read_csv(\"Data Skripsi Anne NoTgl.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526f3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cod_train2, test= train_test_split(cod_train, random_state=0, test_size=0.2)\n",
    "train, val =  train_test_split(cod_train2, random_state = 0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c83627b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752     JAKARTA - Sebanyak 18 pegawai Komisi Pemberant...\n",
       "709     JAKARTA - Pemberlakuan pembatasan kegiatan mas...\n",
       "2958    JAKARTA- Kasus positifCovid-19kembali bertamba...\n",
       "3319    JAKARTA  Â“ Pandemi Covid-19 yang melanda Tanah...\n",
       "844     JAKARTA - Menteri Koordinator Bidang Kemaritim...\n",
       "Name: Isi Berita, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Isi Berita'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f1ecf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/candra/miniconda3/envs/env-wrap/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/candra/miniconda3/envs/env-wrap/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/candra/miniconda3/envs/env-wrap/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "max_features = 100000 # max num words\n",
    "maxlen = 250 \n",
    "embedding_size = 200\n",
    "\n",
    "# create the tokenizer with the maximum number of words to keep, \n",
    "# based on word frequency. \n",
    "# Only the most common num_words-1 words will be kept.\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token = True)\n",
    "\n",
    "train['Isi Berita']=train['Isi Berita'].astype(str)\n",
    "test['Isi Berita']=test['Isi Berita'].astype(str)\n",
    "val['Isi Berita']=val['Isi Berita'].astype(str)\n",
    "\n",
    "# fit the tokenizer on the headlines\n",
    "tokenizer.fit_on_texts(list(train['Isi Berita']))\n",
    "\n",
    "# Transforms each text in texts to a sequence of integers.\n",
    "train_X = tokenizer.texts_to_sequences(train['Isi Berita'])\n",
    "test_X = tokenizer.texts_to_sequences(test['Isi Berita'])\n",
    "val_X = tokenizer.texts_to_sequences(val['Isi Berita'])\n",
    "\n",
    "# transforms a list of num_samples sequences (lists of integers)\n",
    "# into a 2D Numpy array of shape (num_samples, num_timesteps).\n",
    "train_X = pad_sequences(train_X, maxlen = maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen = maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen = maxlen)\n",
    "\n",
    "train_y = train['Label']\n",
    "test_y = test['Label']\n",
    "val_y = val['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a52e5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(X_train, X_test):\n",
    "    raw_docs_train = X_train\n",
    "    raw_docs_test = X_test\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    max_seq_len = 20\n",
    "    num_classes = 1\n",
    "\n",
    "    print(\"pre-processing train data...\")\n",
    "    processed_docs_train = []\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        filtered = [word for word in tokens]\n",
    "        processed_docs_train.append(\" \".join(filtered))\n",
    "\n",
    "    processed_docs_test = []\n",
    "    for doc in tqdm(raw_docs_test):\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        filtered = [word for word in tokens]\n",
    "        processed_docs_test.append(\" \".join(filtered))\n",
    "        \n",
    "    return processed_docs_train, processed_docs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdaf7182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_index_dict(all_processed_data):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(all_processed_data)\n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"dictionary size: \", len(word_index))\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9761b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download cc.id vector https://www.kaggle.com/khotijahs1/ccid300vecgz\n",
    "def load_indonesian_word_embeddings():\n",
    "    import codecs\n",
    "    indonesian_embeddings_index = {}\n",
    "    fasttext_indo = codecs.open('cc.id.300.vec', encoding='utf-8')\n",
    "    for line in tqdm(fasttext_indo):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        indonesian_embeddings_index[word] = coefs\n",
    "\n",
    "    fasttext_indo.close()\n",
    "    print('found %s word vectors' % len(indonesian_embeddings_index))\n",
    "    \n",
    "    return indonesian_embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a781ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding_matrix(word_index, embeddings_index):\n",
    "    embed_dim = 300\n",
    "    words_not_found = []\n",
    "    MAX_NB_WORDS = 30000\n",
    "    \n",
    "    print('preparing embedding matrix...')\n",
    "\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "    \n",
    "    embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "    \n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebef064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [07:28, 4455.75it/s] "
     ]
    }
   ],
   "source": [
    "# EMBEDDING_FILE = 'vocab.txt'\n",
    "\n",
    "# def get_coefs(word,*arr): \n",
    "#     return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "# embeddings_index = {}\n",
    "# with open(EMBEDDING_FILE, encoding=\"utf8\") as f:\n",
    "#     for line in f:\n",
    "#         word, coefs = get_coefs(*line.split(\" \"))\n",
    "#         embeddings_index[word] = coefs\n",
    "            \n",
    "# all_embs = np.stack(embeddings_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "# embed_size = all_embs.shape[1]\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "\n",
    "# embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embedding_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: \n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: \n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "embedding_index = load_indonesian_word_embeddings()\n",
    "train_data_processed, test_data_processed = preprocess_dataset(train_X, test_X)\n",
    "tokenizer = create_word_index_dict(train_data_processed+test_data_processed)\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix, nb_words = plot_embedding_matrix(word_index, embedding_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_size, weights = [embedding_matrix]))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(40, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be006a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model after every epoch.\n",
    "saveBestModel = keras.callbacks.ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "# Stop training when a monitored quantity has stopped improving.\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1860fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 25\n",
    "model.fit(train_X, train_y, batch_size=batch_size, epochs=epochs, validation_data=(val_X, val_y), callbacks=[saveBestModel, earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9df784",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, precision, recall, true_positives = model.evaluate(test_X, test_y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d480b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_pr=precision*recall\n",
    "sum_pr=precision+recall\n",
    "div=mult_pr/sum_pr\n",
    "f1_score=2*div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14736bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss:',loss)\n",
    "print('Accuracy:',accuracy)\n",
    "print('Precision:',precision)\n",
    "print('Recall:',recall)\n",
    "print('f1 score:',f1_score)\n",
    "print('True positives:',true_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714099fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict_classes(test_X, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6128542",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFP_FN_TP_lists(test_X, test_y, pred_y):\n",
    "    FP_text = []\n",
    "    FP_index = []\n",
    "    FN_text = []\n",
    "    FN_index = []\n",
    "    TP_text = []\n",
    "    TP_index = []\n",
    "    for i in range(len(test_y)):\n",
    "        if(pred_y[i]==1 and test_y[test_y.index[i]]==0):\n",
    "            FP_text.append(test['text'][test_y.index[i]])\n",
    "            FP_index.append(test_y.index[i])\n",
    "        elif(pred_y[i]==0 and test_y[test_y.index[i]]==1):\n",
    "            FN_text.append(test['text'][test_y.index[i]])\n",
    "            FN_index.append(test_y.index[i])\n",
    "        elif(pred_y[i]==1 and test_y[test_y.index[i]]==1):\n",
    "            TP_text.append(test['text'][test_y.index[i]])\n",
    "            TP_index.append(test_y.index[i])        \n",
    "            \n",
    "    return FP_text,FP_index,FN_text,FN_index,TP_text,TP_index\n",
    "\n",
    "def getFP_FN_TP(test_X, test_y, pred_y):\n",
    "    FP_text,FP_index,FN_text,FN_index,TP_text,TP_index = getFP_FN_TP_lists(test_X, test_y, pred_y)\n",
    "    d_FP = {'FP_text':FP_text,'FP_index':FP_index}\n",
    "    df_FP = pd.DataFrame(d_FP)\n",
    "    d_FN = {'FN_text':FN_text,'FN_index':FN_index}\n",
    "    df_FN = pd.DataFrame(d_FN)\n",
    "    d_TP =  {'TP_text':TP_text,'TP_index':TP_index}\n",
    "    df_TP = pd.DataFrame(d_TP)\n",
    "    \n",
    "    return df_FP,df_FN,df_TP\n",
    "\n",
    "df_FP,df_FN, df_TP = getFP_FN_TP(test_X, test_y, pred_y)\n",
    "df_FP.to_csv('FP_BiLSTMG_CB.csv', index=True)\n",
    "df_FN.to_csv('FN_BiLSTM_CB.csv', index=True)\n",
    "df_TP.to_csv('TP_BiLSTM_CB.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e764c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
